{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install git+https://github.com/huggingface/speechbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "concatenated_librispeech = load_dataset(\n",
    "    \"sanchit-gandhi/concatenated_librispeech\", split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <F263C8ED-23C1-35DD-BD33-2CD667C0ED1D> /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <E854E4B4-D8A9-321E-9852-69F8F3B956BB> /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\",\n",
    ")\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"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\")\n",
    "\n",
    "from speechbox import ASRDiarizationPipeline\n",
    "pipeline = ASRDiarizationPipeline(\n",
    "    asr_pipeline=asr_pipeline, diarization_pipeline=diarization_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now instantiate our combined diarization plus transcription pipeline, by passing the diarization model and ASR model to the ASRDiarizationPipeline class:\n",
    "def tuple_to_string(start_end_tuple, ndigits=1):\n",
    "    return str((round(start_end_tuple[0], ndigits), round(start_end_tuple[1], ndigits)))\n",
    "\n",
    "\n",
    "def format_as_transcription(raw_segments):\n",
    "    return [chunk for chunk in raw_segments]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt Diarization Outputs to be inputed into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexTranscript():\n",
    "    def __init__(self, transcript):\n",
    "        self.speakers = []\n",
    "        self.texts = []\n",
    "        self.timestamps = []\n",
    "        self.is_conversation = []\n",
    "\n",
    "        for i in range(len(transcript) - 1):\n",
    "            self.speakers.append([transcript[i]['speaker'], transcript[i+1]['speaker']])\n",
    "            self.texts.append([transcript[i]['text'], transcript[i+1]['text']])\n",
    "            self.timestamps.append([transcript[i]['timestamp'], transcript[i+1]['timestamp']])\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.speakers)\n",
    "    \n",
    "    def expire_speakers(self, conversation_members, curr_time):\n",
    "        for speaker, expiry_time in conversation_members.items():\n",
    "            if curr_time > expiry_time:\n",
    "                conversation_members.pop(speaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate file but idk how to separate it ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a \n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self, input_size=2*768, hidden_size=384):\n",
    "        super(ComplexModel, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer 1\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Fully connected layer 2\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size) # Fully connected layer 3\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size) # Fully connected layer 4\n",
    "        self.fc5 = nn.Linear(hidden_size, 1)           # Output layer\n",
    "        \n",
    "        self.relu = nn.ReLU()     # ReLU activation function\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation function\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.4) # Dropout layer to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "device = pt.device(\"mps\") if pt.backends.mps.is_available() else pt.device(\"cpu\")\n",
    "embedding_classifier = pt.load(\"is_conversation_embedding_classifier_doubles.pt\")\n",
    "embedding_classifier.to(device)\n",
    "embedding_classifier.eval()\n",
    "# Step 2: Create the embeddings\n",
    "sent_transf = SentenceTransformer(\"avsolatorio/GIST-Embedding-v0\", revision=None)\n",
    "sent_transf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# Load the MP3 file\n",
    "\n",
    "def return_doubles(filename = \"Audios/Lady Yum.m4a\") -> ComplexTranscript: \n",
    "    audio, sr = librosa.load(filename, sr=16000)\n",
    "    input = []\n",
    "    outputs = pipeline(audio.copy())\n",
    "    transcript = format_as_transcription(outputs)\n",
    "    \n",
    "    fixed_transcript = ComplexTranscript(transcript)\n",
    "    return fixed_transcript\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ComplexTranscript at 0x303c75110>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = return_doubles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubles_to_embeddings(text_double, sent_transf):\n",
    "    raw_embeddings = sent_transf.encode(text_double)\n",
    "    return pt.tensor(np.concatenate((raw_embeddings[0], raw_embeddings[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPEAKER_00', 'SPEAKER_01'] [(0.0, 3.0), (3.0, 5.0)] [' Hello Antonio.', ' Hey!'] 0.9997491240501404\n",
      "['SPEAKER_01', 'SPEAKER_00'] [(3.0, 5.0), (5.0, 7.0)] [' Hey!', ' How are you today?'] 0.996129035949707\n",
      "['SPEAKER_00', 'SPEAKER_01'] [(5.0, 7.0), (7.0, 9.0)] [' How are you today?', \" I'm good. You?\"] 0.9981352090835571\n",
      "['SPEAKER_01', 'SPEAKER_00'] [(7.0, 9.0), (9.0, 12.0)] [\" I'm good. You?\", \" I'm great. What did you do today?\"] 0.9968698620796204\n",
      "['SPEAKER_00', 'SPEAKER_01'] [(9.0, 12.0), (12.0, 15.0)] [\" I'm great. What did you do today?\", ' I went for a long walk.'] 0.8426409959793091\n",
      "['SPEAKER_01', 'SPEAKER_00'] [(12.0, 15.0), (15.0, 17.0)] [' I went for a long walk.', ' Oh, where did you go to?'] 0.9999840259552002\n",
      "['SPEAKER_00', 'SPEAKER_01'] [(15.0, 17.0), (17.0, 19.0)] [' Oh, where did you go to?', ' To the pier.'] 0.9972073435783386\n",
      "['SPEAKER_01', 'SPEAKER_00'] [(17.0, 19.0), (19.0, 21.0)] [' To the pier.', \" Hmm, that's nice.\"] 0.9999958276748657\n"
     ]
    }
   ],
   "source": [
    "for idx, t in enumerate(transcript.texts):\n",
    "    e = doubles_to_embeddings(t, sent_transf)\n",
    "    is_conversation = embedding_classifier(e.to(device))[0].item()\n",
    "    transcript.is_conversation.append(is_conversation)\n",
    "    print(transcript.speakers[idx], transcript.timestamps[idx],t, is_conversation)\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_members: dict[str,float]= {}\n",
    "for i in range(transcript.len()):\n",
    "    if transcript.is_conversation[i]:\n",
    "        for s_idx, s in enumerate(transcript.speakers[i]):\n",
    "            conversation_members[s] = transcript.timestamps[i][s_idx][1] + 30\n",
    "        transcript.expire_speakers(conversation_members, transcript.timestamps[i][-1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER_00 51.0\n",
      "SPEAKER_01 49.0\n"
     ]
    }
   ],
   "source": [
    "for speaker, expiry_time in conversation_members.items():\n",
    "    print(speaker, expiry_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationCluster:\n",
    "    def __init__(self) -> None:\n",
    "        self.members = {}\n",
    "        self.history = []\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me encantaria que asi como se va actualizando la lista de conversation members, \n",
    "# tener algo que yo pueda constantemente estar revisando si la gente nueva esta o no esta en la conversacion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
